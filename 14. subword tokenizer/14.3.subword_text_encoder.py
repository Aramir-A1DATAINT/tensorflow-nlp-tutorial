# -*- coding: utf-8 -*-
"""서브워드 텍스트 인코더.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qII2QkEJRy2qsLs6XZ-Ky08IG7dQnxMA

2021년 10월 9일에 마지막으로 실행 확인하였습니다.  
위키독스 '딥 러닝을 이용한 자연어 처리 입문'의 서브워드 텍스트 인코더 튜토리얼입니다.  

링크 : https://wikidocs.net/86792
"""

import tensorflow as tf
import tensorflow_datasets as tfds

tf.__version__

!wget - q
http: // ai.stanford.edu / ~amaas / data / sentiment / aclImdb_v1.tar.gz
!tar
zxf
aclImdb_v1.tar.gz

import pandas as pd
import glob
import os
import string


def get_dfs(start_path):
    df = pd.DataFrame(columns=['text', 'sent'])
    text = []
    sent = []
    for p in ['pos', 'neg']:
        path = os.path.join(start_path, p)
        files = [f for f in os.listdir(path)
                 if os.path.isfile(os.path.join(path, f))]
        for f in files:
            with open(os.path.join(path, f), "r") as myfile:
                # replace carriage return linefeed with spaces
                text.append(myfile.read()
                            .replace("\n", " ")
                            .replace("\r", " "))
                # convert positive reviews to 1 and negative reviews to zero
                sent.append(1 if p == 'pos' else 0)

    df['text'] = text
    df['sent'] = sent
    # This line shuffles the data so you don't end up with contiguous
    # blocks of positive and negative reviews
    df = df.sample(frac=1).reset_index(drop=True)
    return df


train_df = get_dfs("aclImdb/train/")
test_df = get_dfs("aclImdb/test/")

train_df

train_df['text']

tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(
    train_df['text'], target_vocab_size=2 ** 13)

print(tokenizer.subwords)

print('Tokenized sample question: {}'.format(tokenizer.encode(train_df['text'][20])))

# train_df에 존재하는 문장 중 일부를 발췌
sample_string = "It's mind-blowing to me that this film was even made."

# encode
tokenized_string = tokenizer.encode(sample_string)
print('정수 인코딩 후의 문장 {}'.format(tokenized_string))

# encoding한 문장을 다시 decode
original_string = tokenizer.decode(tokenized_string)
print('기존 문장: {}'.format(original_string))

assert original_string == sample_string

print('단어 집합의 크기(Vocab size) :', tokenizer.vocab_size)

for ts in tokenized_string:
    print('{} ----> {}'.format(ts, tokenizer.decode([ts])))

sample_string = "It's mind-blowing to me that this film was evenxyz made."

# encode
tokenized_string = tokenizer.encode(sample_string)
print('정수 인코딩 후의 문장 {}'.format(tokenized_string))

# encoding한 문장을 다시 decode
original_string = tokenizer.decode(tokenized_string)
print('기존 문장: {}'.format(original_string))

assert original_string == sample_string

for ts in tokenized_string:
    print('{} ----> {}'.format(ts, tokenizer.decode([ts])))

"""네이버 영화 리뷰에 대해서도 위에서 IMDB 영화 리뷰에 대해서 수행한 동일한 작업을 진행해봅시다."""

import urllib.request

urllib.request.urlretrieve("https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt",
                           filename="ratings_train.txt")
urllib.request.urlretrieve("https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt",
                           filename="ratings_test.txt")

train_data = pd.read_table('ratings_train.txt')
test_data = pd.read_table('ratings_test.txt')

train_data[:5]  # 상위 5개 출력

print(train_data.isnull().values.any())

print(train_data.isnull().sum())

train_data = train_data.dropna(how='any')  # Null 값이 존재하는 행 제거
print(train_data.isnull().values.any())  # Null 값이 존재하는지 확인

"""The vocabulary is "trained" on a corpus and all wordpieces are stored in a vocabulary file. To generate a vocabulary from a corpus, use tfds.features.text.SubwordTextEncoder.build_from_corpus."""

tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(
    train_data['document'], target_vocab_size=2 ** 13)

print(tokenizer.subwords[:100])

print('Tokenized sample question: {}'.format(tokenizer.encode(train_data['document'][20])))

sample_string = train_data['document'][21]

# encode
tokenized_string = tokenizer.encode(sample_string)
print('정수 인코딩 후의 문장 {}'.format(tokenized_string))

# encoding한 문장을 다시 decode
original_string = tokenizer.decode(tokenized_string)
print('기존 문장: {}'.format(original_string))

assert original_string == sample_string

for ts in tokenized_string:
    print('{} ----> {}'.format(ts, tokenizer.decode([ts])))

sample_string = '이 영화 굉장히 재밌다 킄핫핫ㅎ'

# encode
tokenized_string = tokenizer.encode(sample_string)
print('정수 인코딩 후의 문장 {}'.format(tokenized_string))

# encoding한 문장을 다시 decode
original_string = tokenizer.decode(tokenized_string)
print('기존 문장: {}'.format(original_string))

assert original_string == sample_string

for ts in tokenized_string:
    print('{} ----> {}'.format(ts, tokenizer.decode([ts])))
